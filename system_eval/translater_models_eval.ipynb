{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8164892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\anaconda3\\envs\\my_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\moham\\anaconda3\\envs\\my_env\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, SeamlessM4Tv2Model\n",
    "from llama_cpp import Llama\n",
    "from langdetect import detect\n",
    "from gtts import gTTS\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb62136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = [\n",
    "    [\"English\",    \"Detect an obstacle, avoid it, and then change color to green.\"],\n",
    "    [\"French\",     \"Détectez un obstacle, évitez-le, puis changez de couleur pour passer au vert.\"],\n",
    "    [\"German\",     \"Erkennen Sie ein Hindernis, umfahren Sie es und wechseln Sie dann die Farbe auf Grün.\"],\n",
    "    [\"Spanish\",    \"Detecten un obstáculo, evítenlo y luego cambien de color a verde.\"],\n",
    "    [\"Italian\",    \"Rilevate un ostacolo, evitatelo e poi cambiate il colore in verde.\"],\n",
    "    [\"Dutch\",      \"Detecteer een obstakel, vermijd het en verander vervolgens de kleur naar groen.\"],\n",
    "    [\"Polish\",     \"Wykryjcie przeszkodę, ominijcie ją, a następnie zmieńcie kolor na zielony.\"],\n",
    "    [\"Portuguese\", \"Detectem um obstáculo, evitem-no e depois mudem a cor para verde.\"],\n",
    "    [\"Swedish\",    \"Upptäck ett hinder, undvik det och byt sedan färg till grönt.\"],\n",
    "\n",
    "\n",
    "    [\"English\",    \"Wander randomly until a target is detected, approach it, and signal achievement by changing color to red.\"],\n",
    "    [\"French\",     \"Errez de manière aléatoire jusqu'à ce qu'une cible soit détectée, approchez-vous et signalez la réussite en changeant la couleur en rouge.\"],\n",
    "    [\"German\",     \"Streift zufällig umher, bis ein Ziel erkannt wird, nähert euch ihm an und signalisiert den Erfolg, indem ihr die Farbe auf Rot ändert.\"],\n",
    "    [\"Spanish\",    \"Deambulen aleatoriamente hasta que se detecte un objetivo, acérquense a él y señalen el logro cambiando el color a rojo.\"],\n",
    "    [\"Italian\",    \"Vagate casualmente finché non viene rilevato un bersaglio, avvicinatevi e segnalate il successo cambiando il colore in rosso.\"],\n",
    "    [\"Dutch\",      \"Zwerft willekeurig totdat een doel wordt gedetecteerd, nader het en geef het bereiken aan door de kleur te veranderen naar rood.\"],\n",
    "    [\"Polish\",     \"Włóczcie się losowo, aż wykryty zostanie cel, podejdźcie do niego i zasygnalizujcie osiągnięcie, zmieniając kolor na czerwony.\"],\n",
    "    [\"Portuguese\", \"Vaguem aleatoriamente até que um alvo seja detectado, aproximem-se dele e sinalizem a conquista mudando a cor para vermelho.\"],\n",
    "    [\"Swedish\",    \"Vandra slumpmässigt tills ett mål upptäcks, närma er det och signalera framgång genom att byta färg till rött.\"],\n",
    "\n",
    "\n",
    "    [\"English\",    \"Check if the path is clear, and form a line at the center.\"],\n",
    "    [\"French\",     \"Vérifiez si le chemin est libre, puis formez une ligne au centre.\"],\n",
    "    [\"German\",     \"Überprüft, ob der Weg frei ist, und bildet eine Linie in der Mitte.\"],\n",
    "    [\"Spanish\",    \"Comprueben si el camino está despejado y formen una línea en el centro.\"],\n",
    "    [\"Italian\",    \"Verificate che il percorso sia libero e formate una linea al centro.\"],\n",
    "    [\"Dutch\",      \"Controleer of het pad vrij is en vorm een lijn in het midden.\"],\n",
    "    [\"Polish\",     \"Sprawdźcie, czy droga jest wolna, i utwórzcie linię pośrodku.\"],\n",
    "    [\"Portuguese\", \"Verifiquem se o caminho está livre e formem uma linha no centro.\"],\n",
    "    [\"Swedish\",    \"Kontrollera om vägen är fri och bilda en linje i mitten.\"],\n",
    "\n",
    "\n",
    "    [\"English\",    \"Find the goal, signal success by changing color to red, and align movement with other swarm agents.\"],\n",
    "    [\"French\",     \"Trouvez l'objectif, signalez la réussite en changeant la couleur en rouge et alignez votre mouvement avec les autres agents de l'essaim.\"],\n",
    "    [\"German\",     \"Findet das Ziel, signalisiert den Erfolg, indem ihr die Farbe auf Rot ändert, und stimmt eure Bewegung mit anderen Schwarmagenten ab.\"],\n",
    "    [\"Spanish\",    \"Encuentren la meta, señalen el éxito cambiando el color a rojo y alineen su movimiento con otros agentes del enjambre.\"],\n",
    "    [\"Italian\",    \"Trovate l'obiettivo, segnalate il successo cambiando il colore in rosso e allineate i vostri movimenti con gli altri agenti dello sciame.\"],\n",
    "    [\"Dutch\",      \"Vind het doel, geef succes aan door de kleur te veranderen naar rood, en stem je beweging af op andere zwermagenten.\"],\n",
    "    [\"Polish\",     \"Znajdźcie cel, zasygnalizujcie sukces, zmieniając kolor na czerwony, i dostosujcie ruch do innych agentów roju.\"],\n",
    "    [\"Portuguese\", \"Encontrem o objetivo, sinalizem o sucesso mudando a cor para vermelho e alinhem o movimento com outros agentes do enxame.\"],\n",
    "    [\"Swedish\",    \"Hitta målet, signalera framgång genom att byta färg till rött och anpassa rörelsen med andra svärmagenter.\"],\n",
    "\n",
    "\n",
    "    [\"English\",    \"Detect the target, and freeze movement upon reaching it.\"],\n",
    "    [\"French\",     \"Détectez la cible et arrêtez tout mouvement en l'atteignant.\"],\n",
    "    [\"German\",     \"Erkennt das Ziel und stoppt die Bewegung, sobald ihr es erreicht.\"],\n",
    "    [\"Spanish\",    \"Detecten el objetivo y detengan todo movimiento al alcanzarlo.\"],\n",
    "    [\"Italian\",    \"Rilevate l'obiettivo e bloccate il movimento una volta raggiunto.\"],\n",
    "    [\"Dutch\",      \"Detecteer het doel en stop de beweging zodra het is bereikt.\"],\n",
    "    [\"Polish\",     \"Wykryjcie cel i zatrzymajcie ruch po jego osiągnięciu.\"],\n",
    "    [\"Portuguese\", \"Detectem o alvo e congelem o movimento ao alcançá-lo.\"],\n",
    "    [\"Swedish\",    \"Upptäck målet och frys rörelsen när ni når det.\"],\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a3d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update results\n",
    "def update_results(results_df, output_path, model, language, input_text, translated_text, model_time, time):\n",
    "    # Create a new DataFrame if results_df is None\n",
    "    if results_df is None:\n",
    "        results_df = pd.DataFrame(columns=[\"Model\", \"Language\", \"Input\", \"Output\", \"Model time\", \"Time (s)\"])\n",
    "    \n",
    "    # Create a new DataFrame for the current entry\n",
    "    new_entry = pd.DataFrame([{\n",
    "        \"Model\": model,\n",
    "        \"Language\": language,\n",
    "        \"Input\": input_text,\n",
    "        \"Output\": translated_text,\n",
    "        \"Model time\": model_time,\n",
    "        \"Time (s)\": time,\n",
    "    }])\n",
    "\n",
    "    # Concatenate the new entry with the existing DataFrame\n",
    "    results_df = pd.concat([results_df, new_entry], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to the specified output path\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19c7459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"facebook/seamless-m4t-v2-large\"\n",
    "model = SeamlessM4Tv2Model.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "# model = .from_pretrained(\"facebook/seamless-m4t-v2-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "214aa14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seamless_model(audio_file):\n",
    "    try:\n",
    "        # 1) Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # 2) Read raw MP3 bytes into a waveform tensor\n",
    "        audio_file.seek(0)\n",
    "        mp3_bytes = io.BytesIO(audio_file.read())\n",
    "        waveform, sr = torchaudio.load(mp3_bytes, format=\"mp3\")  # shape: [channels, samples]\n",
    "\n",
    "        # 3) Convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # 4) Resample to 16 kHz if needed\n",
    "        target_sr = 16000\n",
    "        if sr != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # 5) Convert to float32 NumPy array in [–1, +1]\n",
    "        audio_array = waveform.squeeze(0).numpy().astype(np.float32)\n",
    "\n",
    "        # 6) Prepare inputs for the model\n",
    "        audio_inputs = processor(audios=audio_array, sampling_rate=target_sr, return_tensors=\"pt\")\n",
    "        audio_inputs = {k: v.to(device) for k, v in audio_inputs.items()}\n",
    "\n",
    "        # 7) Inference\n",
    "        start_time = time.time()\n",
    "        output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # 8) Decode\n",
    "        translated_text = processor.batch_decode(output_tokens.sequences, skip_special_tokens=True)[0]\n",
    "        return translated_text, (end_time - start_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during audio translation: {e}\", None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58dde361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text and translate it using SeamlessM4T\n",
    "def process_text_seamless(text):\n",
    "    try:\n",
    "        # Detect language\n",
    "        language = detect(text)\n",
    "\n",
    "        # Convert text to speech and save as an in-memory buffer\n",
    "        tts = gTTS(text=text, lang=language)\n",
    "        audio_buffer = io.BytesIO()\n",
    "        tts.write_to_fp(audio_buffer)\n",
    "        audio_buffer.seek(0)\n",
    "\n",
    "        # Translate the audio\n",
    "        translated_text, model_time = seamless_model(audio_buffer)\n",
    "        return translated_text, model_time\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during text processing: {e}\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98a392a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect an obstacle, avoid it, and then change color to green.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_1692\\1039760007.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_entry], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détectez un obstacle, évitez-le, puis changez de couleur pour passer au vert.\n",
      "Erkennen Sie ein Hindernis, umfahren Sie es und wechseln Sie dann die Farbe auf Grün.\n",
      "Detecten un obstáculo, evítenlo y luego cambien de color a verde.\n",
      "Rilevate un ostacolo, evitatelo e poi cambiate il colore in verde.\n",
      "Detecteer een obstakel, vermijd het en verander vervolgens de kleur naar groen.\n",
      "Wykryjcie przeszkodę, ominijcie ją, a następnie zmieńcie kolor na zielony.\n",
      "Detectem um obstáculo, evitem-no e depois mudem a cor para verde.\n",
      "Upptäck ett hinder, undvik det och byt sedan färg till grönt.\n",
      "Wander randomly until a target is detected, approach it, and signal achievement by changing color to red.\n",
      "Errez de manière aléatoire jusqu'à ce qu'une cible soit détectée, approchez-vous et signalez la réussite en changeant la couleur en rouge.\n",
      "Streift zufällig umher, bis ein Ziel erkannt wird, nähert euch ihm an und signalisiert den Erfolg, indem ihr die Farbe auf Rot ändert.\n",
      "Deambulen aleatoriamente hasta que se detecte un objetivo, acérquense a él y señalen el logro cambiando el color a rojo.\n",
      "Vagate casualmente finché non viene rilevato un bersaglio, avvicinatevi e segnalate il successo cambiando il colore in rosso.\n",
      "Zwerft willekeurig totdat een doel wordt gedetecteerd, nader het en geef het bereiken aan door de kleur te veranderen naar rood.\n",
      "Włóczcie się losowo, aż wykryty zostanie cel, podejdźcie do niego i zasygnalizujcie osiągnięcie, zmieniając kolor na czerwony.\n",
      "Vaguem aleatoriamente até que um alvo seja detectado, aproximem-se dele e sinalizem a conquista mudando a cor para vermelho.\n",
      "Vandra slumpmässigt tills ett mål upptäcks, närma er det och signalera framgång genom att byta färg till rött.\n",
      "Check if the path is clear, and form a line at the center.\n",
      "Vérifiez si le chemin est libre, puis formez une ligne au centre.\n",
      "Überprüft, ob der Weg frei ist, und bildet eine Linie in der Mitte.\n",
      "Comprueben si el camino está despejado y formen una línea en el centro.\n",
      "Verificate che il percorso sia libero e formate una linea al centro.\n",
      "Controleer of het pad vrij is en vorm een lijn in het midden.\n",
      "Sprawdźcie, czy droga jest wolna, i utwórzcie linię pośrodku.\n",
      "Verifiquem se o caminho está livre e formem uma linha no centro.\n",
      "Kontrollera om vägen är fri och bilda en linje i mitten.\n",
      "Find the goal, signal success by changing color to red, and align movement with other swarm agents.\n",
      "Trouvez l'objectif, signalez la réussite en changeant la couleur en rouge et alignez votre mouvement avec les autres agents de l'essaim.\n",
      "Findet das Ziel, signalisiert den Erfolg, indem ihr die Farbe auf Rot ändert, und stimmt eure Bewegung mit anderen Schwarmagenten ab.\n",
      "Encuentren la meta, señalen el éxito cambiando el color a rojo y alineen su movimiento con otros agentes del enjambre.\n",
      "Trovate l'obiettivo, segnalate il successo cambiando il colore in rosso e allineate i vostri movimenti con gli altri agenti dello sciame.\n",
      "Vind het doel, geef succes aan door de kleur te veranderen naar rood, en stem je beweging af op andere zwermagenten.\n",
      "Znajdźcie cel, zasygnalizujcie sukces, zmieniając kolor na czerwony, i dostosujcie ruch do innych agentów roju.\n",
      "Encontrem o objetivo, sinalizem o sucesso mudando a cor para vermelho e alinhem o movimento com outros agentes do enxame.\n",
      "Hitta målet, signalera framgång genom att byta färg till rött och anpassa rörelsen med andra svärmagenter.\n",
      "Detect the target, and freeze movement upon reaching it.\n",
      "Détectez la cible et arrêtez tout mouvement en l'atteignant.\n",
      "Erkennt das Ziel und stoppt die Bewegung, sobald ihr es erreicht.\n",
      "Detecten el objetivo y detengan todo movimiento al alcanzarlo.\n",
      "Rilevate l'obiettivo e bloccate il movimento una volta raggiunto.\n",
      "Detecteer het doel en stop de beweging zodra het is bereikt.\n",
      "Wykryjcie cel i zatrzymajcie ruch po jego osiągnięciu.\n",
      "Detectem o alvo e congelem o movimento ao alcançá-lo.\n",
      "Upptäck målet och frys rörelsen när ni når det.\n",
      "seamless-m4t-v2-large done\n"
     ]
    }
   ],
   "source": [
    "output_file = r\"./results/seamless-m4t-v2-large-results.csv\"\n",
    "existing_df = pd.read_csv(output_file) if os.path.exists(output_file) else None\n",
    "\n",
    "\n",
    "for language, text in input_text:\n",
    "    print(text)\n",
    "    start_time = time.time()\n",
    "    translated_text, model_time = process_text_seamless(str(text))\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Append a new entry to the results DataFrame\n",
    "    existing_df = update_results(  \n",
    "        results_df=existing_df,\n",
    "        output_path=output_file,\n",
    "        model=\"seamless-m4t-v2-large\",\n",
    "        language=language,\n",
    "        input_text=text,\n",
    "        translated_text=translated_text,\n",
    "        model_time=model_time,\n",
    "        time=end_time - start_time \n",
    "    )\n",
    "\n",
    "print(\"seamless-m4t-v2-large done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142cef9",
   "metadata": {},
   "source": [
    "### Text Processing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53295967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 41 key-value pairs and 381 tensors from G:\\Inventors Hub Projects\\SwarmChat\\models\\EuroLLM-9B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = EuroLLM 9B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = EuroLLM\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 9B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = EuroLLM 9B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Utter Project\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/utter-project/...\n",
      "llama_model_loader: - kv  11:                          general.languages arr[str,35]      = [\"en\", \"de\", \"es\", \"fr\", \"it\", \"pt\", ...\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 42\n",
      "llama_model_loader: - kv  13:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv  14:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  15:                  llama.feed_forward_length u32              = 12288\n",
      "llama_model_loader: - kv  16:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  17:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  19:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  21:                           llama.vocab_size u32              = 128000\n",
      "llama_model_loader: - kv  22:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,128000]  = [\"<unk>\", \"<s>\", \"</s>\", \"<|im_start|...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,128000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,128000]  = [3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, ...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 4\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  35:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  37:                      quantize.imatrix.file str              = /models_out/EuroLLM-9B-Instruct-GGUF/...\n",
      "llama_model_loader: - kv  38:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  39:             quantize.imatrix.entries_count i32              = 294\n",
      "llama_model_loader: - kv  40:              quantize.imatrix.chunks_count i32              = 142\n",
      "llama_model_loader: - type  f32:   85 tensors\n",
      "llama_model_loader: - type q4_K:  253 tensors\n",
      "llama_model_loader: - type q6_K:   43 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      0 '<unk>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      3 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 261\n",
      "llm_load_vocab: token to piece cache size = 0.8186 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 128000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 42\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 12288\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 9.15 B\n",
      "llm_load_print_meta: model size       = 5.20 GiB (4.88 BPW) \n",
      "llm_load_print_meta: general.name     = EuroLLM 9B Instruct\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 4 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 4 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 271 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 4 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 380 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  5321.27 MiB\n",
      "..........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 1024\n",
      "llama_new_context_with_model: n_ctx_per_seq = 1024\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (1024) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 42, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   168.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  168.00 MiB, K (f16):   84.00 MiB, V (f16):   84.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1350\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'EuroLLM 9B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'general.basename': 'EuroLLM', 'general.finetune': 'Instruct', 'general.size_label': '9B', 'general.license': 'apache-2.0', 'general.base_model.count': '1', 'general.base_model.0.name': 'EuroLLM 9B', 'general.base_model.0.organization': 'Utter Project', 'general.base_model.0.repo_url': 'https://huggingface.co/utter-project/EuroLLM-9B', 'llama.block_count': '42', 'llama.context_length': '4096', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '12288', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '4', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '10000.000000', 'quantize.imatrix.entries_count': '294', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128000', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'assistant' %}{% set role = 'assistant' %}{% else %}{% set role = message['role'] %}{% endif %}<|im_start|>{{ role }}\\n{{ message['content'] | trim }}<|im_end|>\\n{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n'}}{% endif %}\", 'quantize.imatrix.chunks_count': '142', 'quantize.imatrix.file': '/models_out/EuroLLM-9B-Instruct-GGUF/EuroLLM-9B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'assistant' %}{% set role = 'assistant' %}{% else %}{% set role = message['role'] %}{% endif %}<|im_start|>{{ role }}\n",
      "{{ message['content'] | trim }}<|im_end|>\n",
      "{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "# Load EuroLLM models\n",
    "model_path_9b = r\"G:\\Inventors Hub Projects\\SwarmChat\\models\\EuroLLM-9B-Instruct-Q4_K_M.gguf\"\n",
    "llm_9b = Llama(model_path=model_path_9b, n_ctx=1024)\n",
    "\n",
    "# Function to process text using EuroLLM\n",
    "def process_text_EuroLLM_9B(text, llm):\n",
    "    input_prompt = f\"\"\"\n",
    "    <|im_start|>system\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Translate the following text to English:\n",
    "    Text: {text}\n",
    "    English: \n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    output = llm(input_prompt, max_tokens=1024, temperature=0)\n",
    "    # print('\\n\\nOutput: ',output,'\\n\\n')\n",
    "    full_response = output.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "    # print('\\n\\nResponse: ',full_response,'\\n\\n')\n",
    "    return full_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dfe9d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 55 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3532.97 ms /    17 tokens\n",
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_9424\\1039760007.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_entry], ignore_index=True)\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6089.95 ms /    51 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6040.78 ms /    53 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5601.69 ms /    49 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5679.66 ms /    50 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5377.38 ms /    48 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    38 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5908.25 ms /    54 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5676.26 ms /    50 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5463.85 ms /    47 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6908.96 ms /    56 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    52 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8202.23 ms /    74 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 51 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    51 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8475.71 ms /    75 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    47 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8869.65 ms /    69 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    43 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8283.54 ms /    64 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    46 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8556.03 ms /    72 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    52 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8495.52 ms /    76 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    47 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8204.37 ms /    69 tokens\n",
      "Llama.generate: 27 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    45 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7477.42 ms /    66 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5222.32 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5323.61 ms /    48 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5179.55 ms /    47 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5103.93 ms /    47 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4886.04 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4902.64 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 34 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    34 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5349.10 ms /    49 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4995.32 ms /    46 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4927.43 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6956.91 ms /    58 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    49 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8631.11 ms /    75 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    47 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8274.56 ms /    70 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    44 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8209.77 ms /    69 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    46 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8580.54 ms /    72 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    42 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7777.78 ms /    66 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    47 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8202.72 ms /    71 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    44 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8098.93 ms /    69 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    45 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8436.39 ms /    71 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4425.28 ms /    40 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4790.92 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5285.83 ms /    46 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4592.61 ms /    42 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4375.45 ms /    41 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4643.94 ms /    43 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4346.15 ms /    41 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5037.55 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   11502.09 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5341.22 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EuroLLM-9B-results updated_df\n"
     ]
    }
   ],
   "source": [
    "output_file = r\"./results/EuroLLM-9B-results.csv\"\n",
    "existing_df = pd.read_csv(output_file) if os.path.exists(output_file) else None\n",
    "\n",
    "\n",
    "\n",
    "for language, text in input_text:\n",
    "    start_time = time.time()\n",
    "    translated_text = process_text_EuroLLM_9B(str(text), llm_9b)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Append a new entry to the results DataFrame\n",
    "    existing_df = update_results(\n",
    "        results_df=existing_df,\n",
    "        output_path=output_file,\n",
    "        model=\"EuroLLM-9B\",\n",
    "        language=language,\n",
    "        input_text=text,\n",
    "        translated_text=translated_text,\n",
    "        model_time=None,\n",
    "        time=end_time - start_time \n",
    "    )\n",
    "\n",
    "print(\"EuroLLM-9B-results updated_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
