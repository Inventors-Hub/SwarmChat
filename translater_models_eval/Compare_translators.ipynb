{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, SeamlessM4Tv2Model\n",
    "from llama_cpp import Llama\n",
    "from langdetect import detect\n",
    "from gtts import gTTS\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\n",
    "    [\"English\", \"Spread out and search for heat signatures.\"],\n",
    "    [\"French\", \"Répartissez-vous et recherchez des signatures thermiques.\"],\n",
    "    [\"German\", \"Verteilen Sie sich und suchen Sie nach Wärmesignaturen.\"],\n",
    "    [\"Spanish\", \"Distribúyanse y busquen firmas de calor.\"],\n",
    "    [\"Italian\", \"Sparpagliatevi e cercate firme di calore.\"],\n",
    "    [\"Dutch\", \"Spreid uit en zoek naar warmtehandtekeningen.\"],\n",
    "    [\"Polish\", \"Rozproszyć się i szukać sygnatur cieplnych.\"],\n",
    "    [\"Portuguese\", \"Espalhem-se e procurem por assinaturas de calor.\"],\n",
    "    [\"Swedish\", \"Sprid ut er och sök efter värmesignaturer.\"],\n",
    "\n",
    "    [\"English\", \"Mark the location of survivors and form a path to guide rescue workers.\"],\n",
    "    [\"French\", \"Marquez l'emplacement des survivants et formez un chemin pour guider les secours.\"],\n",
    "    [\"German\", \"Markieren Sie den Standort der Überlebenden und bilden Sie einen Weg, um Rettungskräfte zu leiten.\"],\n",
    "    [\"Spanish\", \"Marquen la ubicación de los supervivientes y formen un camino para guiar a los rescatistas.\"],\n",
    "    [\"Italian\", \"Segnate la positione dei sopravvissuti e formate un percorso per guidare i soccorritori.\"],\n",
    "    [\"Dutch\", \"Markeer de locatie van overlevenden en vorm een pad om reddingswerkers te begeleiden.\"],\n",
    "    [\"Polish\", \"Oznacz miejsce przebywania ocalałych i wytycz ścieżkę, aby poprowadzić ratowników.\"],\n",
    "    [\"Portuguese\", \"Marque a localização dos sobreviventes e forme um caminho para guiar os socorristas.\"],\n",
    "    [\"Swedish\", \"Markera platsen för överlevande och skapa en väg för att guida räddningsarbetare.\"],\n",
    "\n",
    "    [\"English\", \"Scan the field for dry or unhealthy plants.\"],\n",
    "    [\"French\", \"Scannez le champ à la recherche de plantes sèches ou malades.\"],\n",
    "    [\"German\", \"Scannen Sie das Feld nach trockenen oder kranken Pflanzen.\"],\n",
    "    [\"Spanish\", \"Escaneen el campo en busca de plantas secas o enfermas.\"],\n",
    "    [\"Italian\", \"Scansionate il campo alla ricerca di piante secche o malsane.\"],\n",
    "    [\"Dutch\", \"Scan het veld op droge of ongezonde planten.\"],\n",
    "    [\"Polish\", \"Zeskanuj pole w poszukiwaniu suchych lub chorych roślin.\"],\n",
    "    [\"Portuguese\", \"Escaneie o campo em busca de plantas secas ou doentes.\"],\n",
    "    [\"Swedish\", \"Skanna fältet efter torra eller ohälsosamma växter.\"],\n",
    "\n",
    "    [\"English\", \"Cluster around unhealthy plants and emit a signal for manual inspection.\"],\n",
    "    [\"French\", \"Regroupez-vous autour des plantes malades et émettez un signal pour une inspection manuelle.\"],\n",
    "    [\"German\", \"Gruppieren Sie sich um kranke Pflanzen und senden Sie ein Signal für eine manuelle Inspektion.\"],\n",
    "    [\"Spanish\", \"Agrúpense alrededor de las plantas enfermas y emitan una señal para una inspección manual.\"],\n",
    "    [\"Italian\", \"Raggruppatevi intorno alle piante malsane ed emettete un segnale per un'ispezione manuale.\"],\n",
    "    [\"Dutch\", \"Groep rond ongezonde planten en zend een signaal voor handmatige inspectie.\"],\n",
    "    [\"Polish\", \"Skup się wokół chorych roślin i emituj sygnał do ręcznej inspekcji.\"],\n",
    "    [\"Portuguese\", \"Agrupe-se ao redor das plantas doentes e emita um sinal para inspeção manual.\"],\n",
    "    [\"Swedish\", \"Samlas runt ohälsosamma växter och sänd en signal för manuell inspektion.\"],\n",
    "\n",
    "    [\"English\", \"Search for plastic waste and collect it in one location.\"],\n",
    "    [\"French\", \"Recherchez les déchets plastiques et rassemblez-les en un seul endroit.\"],\n",
    "    [\"German\", \"Suchen Sie nach Plastikmüll und sammeln Sie ihn an einem Ort.\"],\n",
    "    [\"Spanish\", \"Busquen residuos plásticos y júntenlos en un solo lugar.\"],\n",
    "    [\"Italian\", \"Cercate i rifiuti di plastica e raccoglieteli in un unico posto.\"],\n",
    "    [\"Dutch\", \"Zoek naar plastic afval en verzamel het op één locatie.\"],\n",
    "    [\"Polish\", \"Szukaj plastikowych odpadów i zbierz je w jednym miejscu.\"],\n",
    "    [\"Portuguese\", \"Procure por resíduos plásticos e reúna-os em um único local.\"],\n",
    "    [\"Swedish\", \"Sök efter plastavfall och samla det på en plats.\"],\n",
    "\n",
    "    [\"English\", \"After cleaning, form a perimeter around the park.\"],\n",
    "    [\"French\", \"Après le nettoyage, formez un périmètre autour du parc.\"],\n",
    "    [\"German\", \"Bildet nach der Reinigung einen Perimeter um den Park.\"],\n",
    "    [\"Spanish\", \"Después de limpiar, formen un perímetro alrededor del parque.\"],\n",
    "    [\"Italian\", \"Dopo la pulizia, formate un perimetro attorno al parco.\"],\n",
    "    [\"Dutch\", \"Vorm na het schoonmaken een perimeter rond het park.\"],\n",
    "    [\"Polish\", \"Po sprzątaniu utwórz obwód wokół parku.\"],\n",
    "    [\"Portuguese\", \"Após a limpeza, forme um perímetro ao redor do parque.\"],\n",
    "    [\"Swedish\", \"Efter rengöring, skapa en perimeter runt parken.\"],\n",
    "\n",
    "    [\"English\", \"Form a foundation grid at these coordinates.\"],\n",
    "    [\"French\", \"Formez une grille de fondation à ces coordonnées.\"],\n",
    "    [\"German\", \"Bildet ein Fundamentraster an diesen Koordinaten.\"],\n",
    "    [\"Spanish\", \"Formen una cuadrícula de base en estas coordenadas.\"],\n",
    "    [\"Italian\", \"Formate una griglia di base a queste coordinate.\"],\n",
    "    [\"Dutch\", \"Vorm een funderingsrooster op deze coördinaten.\"],\n",
    "    [\"Polish\", \"Utwórz siatkę fundamentów w tych współrzędnych.\"],\n",
    "    [\"Portuguese\", \"Forme uma grade de fundação nestas coordenadas.\"],\n",
    "    [\"Swedish\", \"Skapa ett grundläggningsrutnät vid dessa koordinater.\"],\n",
    "\n",
    "    [\"English\", \"Illuminate the site from above.\"],\n",
    "    [\"French\", \"Illuminez le site depuis les airs.\"],\n",
    "    [\"German\", \"Erleuchten Sie die Stelle von oben.\"],\n",
    "    [\"Spanish\", \"Iluminen el sitio desde arriba.\"],\n",
    "    [\"Italian\", \"Illumina il sito dall'alto.\"],\n",
    "    [\"Dutch\", \"Verlicht de locatie van bovenaf.\"],\n",
    "    [\"Polish\", \"Oświetl miejsce z góry.\"],\n",
    "    [\"Portuguese\", \"Ilumine o local de cima.\"],\n",
    "    [\"Swedish\", \"Belys platsen uppifrån.\"],\n",
    "\n",
    "    [\"English\", \"Form a large circle and flash lights in sync with the music.\"],\n",
    "    [\"French\", \"Formez un grand cercle et clignotez les lumières en rythme avec la musique.\"],\n",
    "    [\"German\", \"Bilden Sie einen großen Kreis und blinken Sie die Lichter im Takt der Musik.\"],\n",
    "    [\"Spanish\", \"Formen un gran círculo y parpadeen las luces al ritmo de la música.\"],\n",
    "    [\"Italian\", \"Formate un grande cerchio e lampeggiate le luci a ritmo di musica.\"],\n",
    "    [\"Dutch\", \"Vorm een grote cirkel en knipper met de lichten in sync met de muziek.\"],\n",
    "    [\"Polish\", \"Utwórz duży okrąg i migaj światłami w rytm muzyki.\"],\n",
    "    [\"Portuguese\", \"Forme um grande círculo e pisque as luzes em sincronia com a música.\"],\n",
    "    [\"Swedish\", \"Skapa en stor cirkel och blinka med lamporna i takt med musiken.\"],\n",
    "\n",
    "    [\"English\", \"Morph into a star shape and rotate clockwise.\"],\n",
    "    [\"French\", \"Formez une étoile et tournez dans le sens des aiguilles d'une montre.\"],\n",
    "    [\"German\", \"Bilden Sie einen Stern und drehen Sie sich im Uhrzeigersinn.\"],\n",
    "    [\"Spanish\", \"Formen una estrella y giren en el sentido de las agujas del reloj.\"],\n",
    "    [\"Italian\", \"Formate una stella e ruotate in senso orario.\"],\n",
    "    [\"Dutch\", \"Vorm een ster en draai met de klok mee.\"],\n",
    "    [\"Polish\", \"Utwórz gwiazdę i obróć zgodnie z ruchem wskazówek zegara.\"],\n",
    "    [\"Portuguese\", \"Forme uma estrela e gire no sentido horário.\"],\n",
    "    [\"Swedish\", \"Bild en stjärna och rotera medsols.\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update results\n",
    "def update_results(results_df, output_path, model, language, input_text, translated_text, model_time, time):\n",
    "    # Create a new DataFrame if results_df is None\n",
    "    if results_df is None:\n",
    "        results_df = pd.DataFrame(columns=[\"Model\", \"Language\", \"Input\", \"Output\", \"Model time\", \"Time (s)\"])\n",
    "    \n",
    "    # Create a new DataFrame for the current entry\n",
    "    new_entry = pd.DataFrame([{\n",
    "        \"Model\": model,\n",
    "        \"Language\": language,\n",
    "        \"Input\": input_text,\n",
    "        \"Output\": translated_text,\n",
    "        \"Model time\": model_time,\n",
    "        \"Time (s)\": time,\n",
    "    }])\n",
    "\n",
    "    # Concatenate the new entry with the existing DataFrame\n",
    "    results_df = pd.concat([results_df, new_entry], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to the specified output path\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speech processing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup device and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-medium\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Function to translate audio using Whisper\n",
    "def translate_audio(audio_file):\n",
    "    try:\n",
    "        if isinstance(audio_file, io.BytesIO):\n",
    "            audio_file.seek(0)\n",
    "            audio = AudioSegment.from_file(audio_file, format=\"mp3\")\n",
    "            audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "            audio_array = np.array(audio.get_array_of_samples()).astype(np.float32) / 32768.0\n",
    "            audio_file = {\"array\": audio_array, \"sampling_rate\": 16000}\n",
    "        elif isinstance(audio_file, str):\n",
    "            pass\n",
    "        else:\n",
    "            return None, None  # Ensure consistent return\n",
    "\n",
    "        start_time = time.time()\n",
    "        input_features = processor(audio_file[\"array\"], sampling_rate=audio_file[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "        input_features = input_features.to(device)\n",
    "        forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"translate\")\n",
    "        generated_tokens = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "        end_time = time.time()\n",
    "        translated_text = processor.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        return translated_text, end_time - start_time\n",
    "    except Exception as e:\n",
    "        return f\"Error during audio translation: {e}\", None\n",
    "\n",
    "# Function to process text using Whisper\n",
    "def process_text_whisper(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        tts = gTTS(text=text, lang=language)\n",
    "        audio_buffer = io.BytesIO()\n",
    "        tts.write_to_fp(audio_buffer)\n",
    "        audio_buffer.seek(0)\n",
    "        translated_text, model_time = translate_audio(audio_buffer)\n",
    "        return translated_text, model_time\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "output_file = \"whisper-medium-results.csv\"\n",
    "existing_df = pd.read_csv(output_file) if os.path.exists(output_file) else None\n",
    "\n",
    "for language, text in input_text:\n",
    "    start_time = time.time()\n",
    "    translated_text, model_time = process_text_whisper(str(text))\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Append a new entry to the results DataFrame\n",
    "    existing_df = update_results(  # Update `existing_df` directly\n",
    "        results_df=existing_df,\n",
    "        output_path=output_file,\n",
    "        model=\"whisper-medium\",\n",
    "        language=language,\n",
    "        input_text=text,\n",
    "        translated_text=translated_text,\n",
    "        model_time=model_time,\n",
    "        time=end_time - start_time \n",
    "    )\n",
    "\n",
    "print(\"whisper-medium done\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "\n",
    "def seamless_model(audio_file):\n",
    "    try:\n",
    "        # Set the device (use GPU if available)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Reset audio file pointer and load audio\n",
    "        audio_file.seek(0)\n",
    "        audio = AudioSegment.from_file(audio_file, format=\"mp3\")\n",
    "        audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "\n",
    "        # Convert audio to float32 NumPy array\n",
    "        audio_array = np.array(audio.get_array_of_samples()).astype(np.float32) / 32768.0\n",
    "\n",
    "        # Process input\n",
    "        audio_inputs = processor(audios=audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        audio_inputs = {key: val.to(device) for key, val in audio_inputs.items()}  # Ensure tensors are on the correct device\n",
    "        start_time = time.time()\n",
    "        # Generate translation\n",
    "        output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
    "\n",
    "        # Extract token IDs from the generated output\n",
    "        token_ids = output_tokens.sequences\n",
    "        # Decode token IDs to text\n",
    "        translated_text_from_audio = processor.batch_decode(token_ids, skip_special_tokens=True)[0]\n",
    "        end_time = time.time()\n",
    "        return translated_text_from_audio, end_time - start_time\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during audio translation: {e}\"\n",
    "\n",
    "\n",
    "# Function to process text and translate it using SeamlessM4T\n",
    "def process_text_seamless(text):\n",
    "    try:\n",
    "        # Detect language\n",
    "        language = detect(text)\n",
    "\n",
    "        # Convert text to speech and save as an in-memory buffer\n",
    "        tts = gTTS(text=text, lang=language)\n",
    "        audio_buffer = io.BytesIO()\n",
    "        tts.write_to_fp(audio_buffer)\n",
    "        audio_buffer.seek(0)\n",
    "\n",
    "        # Translate the audio\n",
    "        translated_text, model_time = seamless_model(audio_buffer)\n",
    "        return translated_text, model_time\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during text processing: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"seamless-m4t-v2-large-results.csv\"\n",
    "existing_df = pd.read_csv(output_file) if os.path.exists(output_file) else None\n",
    "\n",
    "\n",
    "for language, text in input_text:\n",
    "    start_time = time.time()\n",
    "    translated_text, model_time = process_text_seamless(str(text))\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Append a new entry to the results DataFrame\n",
    "    existing_df = update_results(  # Update existing_df with the latest DataFrame\n",
    "        results_df=existing_df,\n",
    "        output_path=output_file,\n",
    "        model=\"seamless-m4t-v2-large\",\n",
    "        language=language,\n",
    "        input_text=text,\n",
    "        translated_text=translated_text,\n",
    "        model_time=model_time,\n",
    "        time=end_time - start_time \n",
    "    )\n",
    "\n",
    "print(\"seamless-m4t-v2-large done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EuroLLM models\n",
    "model_path_1b = r\"model\\eurollm-1.7b-instruct-q4_k_m.gguf\"\n",
    "llm_1b = Llama(model_path=model_path_1b, n_ctx=1024)\n",
    "\n",
    "# Function to process text using EuroLLM\n",
    "def process_text_EuroLLM_1B(text, llm):\n",
    "    input_prompt = f\"\"\"\n",
    "    <|im_start|>system\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Translate the following text to English:\n",
    "    Text: {text}\n",
    "    English: \n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    output = llm(input_prompt, max_tokens=1024, temperature=0)\n",
    "    # print('\\n\\nOutput: ',output,'\\n\\n')\n",
    "    full_response = output.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "    # print('\\n\\nResponse: ',full_response,'\\n\\n')\n",
    "    marker = \"English:\"\n",
    "    if marker in full_response:\n",
    "        translated_text = full_response.split(marker)[-1].strip()\n",
    "        return translated_text if translated_text else \"No valid translation provided.\"\n",
    "    return \"No valid translation provided.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"Eurollm-1.7b-results.csv\"\n",
    "existing_df = pd.read_csv(output_file) if os.path.exists(output_file) else None\n",
    "\n",
    "\n",
    "for language, text in input_text:\n",
    "    start_time = time.time()\n",
    "    translated_text = process_text_EuroLLM_1B(str(text), llm_1b)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Append a new entry to the results DataFrame\n",
    "    existing_df = update_results(\n",
    "        results_df=existing_df,\n",
    "        output_path=output_file,\n",
    "        model=\"Eurollm-1.7b\",\n",
    "        language=language,\n",
    "        input_text=text,\n",
    "        translated_text=translated_text,\n",
    "        model_time=None,\n",
    "        time=end_time - start_time \n",
    "    )\n",
    "\n",
    "print(\"Eurollm-1.7b updated_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 41 key-value pairs and 381 tensors from model\\EuroLLM-9B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = EuroLLM 9B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = EuroLLM\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 9B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = EuroLLM 9B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Utter Project\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/utter-project/...\n",
      "llama_model_loader: - kv  11:                          general.languages arr[str,35]      = [\"en\", \"de\", \"es\", \"fr\", \"it\", \"pt\", ...\n",
      "llama_model_loader: - kv  12:                          llama.block_count u32              = 42\n",
      "llama_model_loader: - kv  13:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv  14:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  15:                  llama.feed_forward_length u32              = 12288\n",
      "llama_model_loader: - kv  16:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  17:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  19:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  21:                           llama.vocab_size u32              = 128000\n",
      "llama_model_loader: - kv  22:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,128000]  = [\"<unk>\", \"<s>\", \"</s>\", \"<|im_start|...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,128000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,128000]  = [3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, ...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 4\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  35:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  37:                      quantize.imatrix.file str              = /models_out/EuroLLM-9B-Instruct-GGUF/...\n",
      "llama_model_loader: - kv  38:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  39:             quantize.imatrix.entries_count i32              = 294\n",
      "llama_model_loader: - kv  40:              quantize.imatrix.chunks_count i32              = 142\n",
      "llama_model_loader: - type  f32:   85 tensors\n",
      "llama_model_loader: - type q4_K:  253 tensors\n",
      "llama_model_loader: - type q6_K:   43 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      0 '<unk>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      3 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 261\n",
      "llm_load_vocab: token to piece cache size = 0.8186 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 128000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 42\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 12288\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 9.15 B\n",
      "llm_load_print_meta: model size       = 5.20 GiB (4.88 BPW) \n",
      "llm_load_print_meta: general.name     = EuroLLM 9B Instruct\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 4 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 4 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 271 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 4 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 380 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  5321.27 MiB\n",
      "..........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 1024\n",
      "llama_new_context_with_model: n_ctx_per_seq = 1024\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (1024) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 42, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   168.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  168.00 MiB, K (f16):   84.00 MiB, V (f16):   84.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1350\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'EuroLLM 9B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'general.basename': 'EuroLLM', 'general.finetune': 'Instruct', 'general.size_label': '9B', 'general.license': 'apache-2.0', 'general.base_model.count': '1', 'general.base_model.0.name': 'EuroLLM 9B', 'general.base_model.0.organization': 'Utter Project', 'general.base_model.0.repo_url': 'https://huggingface.co/utter-project/EuroLLM-9B', 'llama.block_count': '42', 'llama.context_length': '4096', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '12288', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '4', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '10000.000000', 'quantize.imatrix.entries_count': '294', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128000', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'assistant' %}{% set role = 'assistant' %}{% else %}{% set role = message['role'] %}{% endif %}<|im_start|>{{ role }}\\n{{ message['content'] | trim }}<|im_end|>\\n{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n'}}{% endif %}\", 'quantize.imatrix.chunks_count': '142', 'quantize.imatrix.file': '/models_out/EuroLLM-9B-Instruct-GGUF/EuroLLM-9B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'assistant' %}{% set role = 'assistant' %}{% else %}{% set role = message['role'] %}{% endif %}<|im_start|>{{ role }}\n",
      "{{ message['content'] | trim }}<|im_end|>\n",
      "{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "# Load EuroLLM models\n",
    "model_path_9b = r\"model\\EuroLLM-9B-Instruct-Q4_K_M.gguf\"\n",
    "llm_9b = Llama(model_path=model_path_9b, n_ctx=1024)\n",
    "\n",
    "# Function to process text using EuroLLM\n",
    "def process_text_EuroLLM_9B(text, llm):\n",
    "    input_prompt = f\"\"\"\n",
    "    <|im_start|>system\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Translate the following text to English:\n",
    "    Text: {text}\n",
    "    English: \n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "    output = llm(input_prompt, max_tokens=1024, temperature=0)\n",
    "    # print('\\n\\nOutput: ',output,'\\n\\n')\n",
    "    full_response = output.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "    # print('\\n\\nResponse: ',full_response,'\\n\\n')\n",
    "    return full_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    50 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   21663.75 ms /    59 tokens\n",
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_10952\\1039760007.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_entry], ignore_index=True)\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4288.65 ms /    38 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3753.79 ms /    37 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3762.01 ms /    36 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4194.44 ms /    38 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3957.75 ms /    35 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4048.21 ms /    36 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3920.92 ms /    38 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4213.38 ms /    38 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4979.13 ms /    43 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6310.17 ms /    51 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    38 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5695.94 ms /    53 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6038.25 ms /    53 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6198.26 ms /    56 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5325.62 ms /    50 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6063.45 ms /    53 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 34 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    34 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5855.13 ms /    51 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5366.93 ms /    50 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4288.22 ms /    39 tokens\n",
      "Llama.generate: 27 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4039.05 ms /    40 tokens\n",
      "Llama.generate: 27 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3973.15 ms /    37 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4417.21 ms /    39 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4555.36 ms /    43 tokens\n",
      "Llama.generate: 27 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4223.97 ms /    37 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4579.71 ms /    41 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4431.05 ms /    40 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4980.02 ms /    44 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5469.10 ms /    44 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    37 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5687.02 ms /    52 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5130.60 ms /    49 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5501.84 ms /    50 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    40 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6245.33 ms /    57 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5107.94 ms /    46 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 34 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    34 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5573.85 ms /    49 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    37 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5379.21 ms /    51 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5518.62 ms /    51 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4271.55 ms /    38 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4598.36 ms /    43 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4682.97 ms /    44 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4546.77 ms /    42 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4344.49 ms /    42 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4416.02 ms /    40 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4522.50 ms /    42 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4720.83 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4419.44 ms /    41 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4074.77 ms /    37 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4355.21 ms /    42 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4151.10 ms /    39 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4204.92 ms /    40 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4274.77 ms /    41 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4156.49 ms /    38 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4174.67 ms /    37 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4308.72 ms /    39 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4227.76 ms /    39 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3308.44 ms /    31 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3743.59 ms /    34 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3395.91 ms /    34 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3445.93 ms /    34 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3396.14 ms /    34 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3560.13 ms /    36 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3547.80 ms /    36 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3344.41 ms /    33 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3568.63 ms /    37 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3333.39 ms /    31 tokens\n",
      "Llama.generate: 27 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3649.99 ms /    33 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3268.86 ms /    32 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3176.17 ms /    30 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3021.84 ms /    31 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3557.10 ms /    32 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3074.73 ms /    29 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3217.86 ms /    30 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2965.46 ms /    29 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4942.69 ms /    43 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5604.14 ms /    49 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 34 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    34 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5476.36 ms /    49 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5397.97 ms /    48 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5337.91 ms /    47 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 34 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    34 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5819.23 ms /    50 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5016.54 ms /    45 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5578.65 ms /    49 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5387.05 ms /    48 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4347.25 ms /    39 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4644.71 ms /    46 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3664.69 ms /    39 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5255.33 ms /    47 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3706.03 ms /    37 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3337.98 ms /    34 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4027.70 ms /    42 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3424.54 ms /    34 tokens\n",
      "Llama.generate: 26 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19564.72 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3893.48 ms /    36 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EuroLLM-9B-results updated_df\n"
     ]
    }
   ],
   "source": [
    "output_file = \"EuroLLM-9B-results.csv\"\n",
    "existing_df = pd.read_csv(output_file) if os.path.exists(output_file) else None\n",
    "\n",
    "\n",
    "\n",
    "for language, text in input_text:\n",
    "    start_time = time.time()\n",
    "    translated_text = process_text_EuroLLM_9B(str(text), llm_9b)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Append a new entry to the results DataFrame\n",
    "    existing_df = update_results(\n",
    "        results_df=existing_df,\n",
    "        output_path=output_file,\n",
    "        model=\"EuroLLM-9B\",\n",
    "        language=language,\n",
    "        input_text=text,\n",
    "        translated_text=translated_text,\n",
    "        model_time=None,\n",
    "        time=end_time - start_time \n",
    "    )\n",
    "\n",
    "print(\"EuroLLM-9B-results updated_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
